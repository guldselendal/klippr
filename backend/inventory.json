{
  "concurrency_inventory": [
    {
      "id": "main-threadpool-1",
      "file": "main.py",
      "line": 8,
      "construct": "ThreadPoolExecutor",
      "type": "thread_pool",
      "purpose": "Parallel summary generation in synchronous mode (disabled by default)",
      "owner": "upload_file, upload_files endpoints",
      "max_workers": "3-8 (env: SUMMARY_MAX_WORKERS)",
      "shared_resources": ["LLM provider", "Database"],
      "synchronization": "as_completed() for result collection",
      "suspected_issue": "Nested with chunk processing, may create 64+ threads",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "summarizer-threadpool-1",
      "file": "summarizer.py",
      "line": 238,
      "construct": "ThreadPoolExecutor",
      "type": "thread_pool",
      "purpose": "Parallel chunk summarization for long chapters",
      "owner": "generate_summary()",
      "max_workers": "min(cpu_count, len(chunks), 16)",
      "shared_resources": ["LLM provider (via semaphore)", "Cache"],
      "synchronization": "as_completed() for result collection",
      "suspected_issue": "Nested inside summary_worker threads, creates 64+ threads total",
      "mvr_critical": true,
      "enabled_by_default": true
    },
    {
      "id": "summarizer-parallel-threadpool-1",
      "file": "summarizer_parallel.py",
      "line": 62,
      "construct": "ThreadPoolExecutor",
      "type": "thread_pool",
      "purpose": "Parallel summary generation for multiple chapters",
      "owner": "generate_summaries_parallel()",
      "max_workers": "min(3, len(chapters), cpu_count, 16)",
      "shared_resources": ["LLM provider (via semaphore)"],
      "synchronization": "as_completed() for result collection",
      "suspected_issue": "May be redundant with main.py ThreadPoolExecutor",
      "mvr_critical": true,
      "enabled_by_default": true
    },
    {
      "id": "summarizer-parallel-threadpool-2",
      "file": "summarizer_parallel.py",
      "line": 123,
      "construct": "ThreadPoolExecutor",
      "type": "thread_pool",
      "purpose": "Parallel title/preview generation",
      "owner": "process_summaries_for_titles_and_previews()",
      "max_workers": "min(3, len(summaries), cpu_count, 16)",
      "shared_resources": ["LLM provider (via semaphore)"],
      "synchronization": "as_completed() for result collection",
      "suspected_issue": "Sequential with summary generation, could be combined",
      "mvr_critical": true,
      "enabled_by_default": true
    },
    {
      "id": "pipeline-threadpool-1",
      "file": "summary_pipeline.py",
      "line": 153,
      "construct": "ThreadPoolExecutor",
      "type": "thread_pool",
      "purpose": "Timeout wrapper for LLM calls",
      "owner": "_call_llm_with_timeout()",
      "max_workers": 1,
      "shared_resources": ["LLM provider"],
      "synchronization": "future.result(timeout=...)",
      "suspected_issue": "Single worker executor adds overhead, may not interrupt blocking I/O",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "pipeline-threadpool-2",
      "file": "summary_pipeline.py",
      "line": 192,
      "construct": "ThreadPoolExecutor",
      "type": "thread_pool",
      "purpose": "Parallel chunk processing in summary_worker",
      "owner": "summary_worker() -> generate_summary_with_routing()",
      "max_workers": "min(16, len(chunks))",
      "shared_resources": ["LLM provider (via semaphore)", "Cache"],
      "synchronization": "as_completed() for result collection",
      "suspected_issue": "Nested inside summary_worker threads, creates 64+ threads",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "pipeline-worker-threads",
      "file": "summary_pipeline.py",
      "line": 754,
      "construct": "threading.Thread",
      "type": "thread",
      "purpose": "Summary generation workers (adaptive scaling)",
      "owner": "initialize_pipeline()",
      "count": "2-8 (adaptive, based on latency/queue)",
      "shared_resources": ["summary_queue", "write_queue", "LLM provider"],
      "synchronization": "queue.Queue.get() with timeout",
      "suspected_issue": "Adaptive scaling may oversubscribe, creates many threads",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "pipeline-db-writer-thread",
      "file": "summary_pipeline.py",
      "line": 830,
      "construct": "threading.Thread",
      "type": "thread",
      "purpose": "Database write batching",
      "owner": "initialize_pipeline()",
      "count": 1,
      "shared_resources": ["write_queue", "Database"],
      "synchronization": "queue.Queue.get() with timeout",
      "suspected_issue": "Single thread may be bottleneck for high throughput",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "pipeline-adaptive-manager-thread",
      "file": "summary_pipeline.py",
      "line": 835,
      "construct": "threading.Thread",
      "type": "thread",
      "purpose": "Adaptive worker scaling",
      "owner": "initialize_pipeline()",
      "count": 1,
      "shared_resources": ["worker_threads list", "llm_latencies deque"],
      "synchronization": "threading.Event for shutdown",
      "suspected_issue": "Adds complexity, may cause thread churn",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "pipeline-queues",
      "file": "summary_pipeline.py",
      "line": 58,
      "construct": "queue.Queue",
      "type": "queue",
      "purpose": "Task queues for async pipeline",
      "owner": "summary_pipeline module",
      "count": 2,
      "maxsize": "200 (summary), 500 (write)",
      "shared_resources": ["worker_threads", "db_writer_thread"],
      "synchronization": "queue.Queue internal locks",
      "suspected_issue": "Bounded queues may cause blocking, backpressure not always handled",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "llm-concurrency-semaphores",
      "file": "llm_concurrency.py",
      "line": 66,
      "construct": "threading.Semaphore",
      "type": "semaphore",
      "purpose": "Limit concurrent LLM API calls",
      "owner": "LLMConcurrencyLimiter",
      "count": 5,
      "limits": "Ollama: 16, Gemini: 8, OpenAI: 10, DeepSeek: 8, Global: 20",
      "shared_resources": ["LLM provider clients"],
      "synchronization": "Two-level semaphore (provider + global)",
      "suspected_issue": "Ollama limit (16) may be too high for local instance, causes degradation",
      "mvr_critical": true,
      "enabled_by_default": true
    },
    {
      "id": "llm-concurrency-lock",
      "file": "llm_concurrency.py",
      "line": 42,
      "construct": "threading.Lock",
      "type": "lock",
      "purpose": "Singleton initialization",
      "owner": "LLMConcurrencyLimiter.__new__()",
      "count": 1,
      "shared_resources": ["_instance"],
      "synchronization": "Double-checked locking pattern",
      "suspected_issue": "Low contention, acceptable",
      "mvr_critical": false,
      "enabled_by_default": true
    },
    {
      "id": "llm-concurrency-metrics-lock",
      "file": "llm_concurrency.py",
      "line": 80,
      "construct": "threading.Lock",
      "type": "lock",
      "purpose": "Protect metrics updates",
      "owner": "LLMConcurrencyLimiter",
      "count": 1,
      "shared_resources": ["in_flight dict", "total_requests", "blocked_requests"],
      "synchronization": "Lock around all metrics updates",
      "suspected_issue": "May cause contention with high concurrency, consider lock-free counters",
      "mvr_critical": false,
      "enabled_by_default": true
    },
    {
      "id": "cache-lock",
      "file": "chunk_cache.py",
      "line": 18,
      "construct": "threading.Lock",
      "type": "lock",
      "purpose": "Protect in-memory cache",
      "owner": "CacheService",
      "count": 1,
      "shared_resources": ["_memory_cache dict"],
      "synchronization": "Lock around cache get/set/delete",
      "suspected_issue": "Low contention, acceptable for MVR",
      "mvr_critical": false,
      "enabled_by_default": true
    },
    {
      "id": "pipeline-shutdown-event",
      "file": "summary_pipeline.py",
      "line": 75,
      "construct": "threading.Event",
      "type": "event",
      "purpose": "Signal shutdown to worker threads",
      "owner": "summary_pipeline module",
      "count": 1,
      "shared_resources": ["worker_threads", "db_writer_thread", "adaptive_manager_thread"],
      "synchronization": "Event.set() to signal, Event.is_set() to check",
      "suspected_issue": "None, standard pattern",
      "mvr_critical": false,
      "enabled_by_default": false
    },
    {
      "id": "fastapi-async-endpoints",
      "file": "main.py",
      "line": 40,
      "construct": "async def",
      "type": "async_function",
      "purpose": "FastAPI async endpoints",
      "owner": "FastAPI app",
      "count": "15+ endpoints",
      "shared_resources": ["Database connection pool"],
      "synchronization": "FastAPI event loop",
      "suspected_issue": "Endpoints are async but call sync code, no true async benefit",
      "mvr_critical": true,
      "enabled_by_default": true
    },
    {
      "id": "database-connection-pool",
      "file": "database.py",
      "line": 22,
      "construct": "QueuePool",
      "type": "connection_pool",
      "purpose": "Database connection pooling",
      "owner": "SQLAlchemy engine",
      "count": "10 connections (PostgreSQL)",
      "shared_resources": ["Database"],
      "synchronization": "SQLAlchemy pool internal locks",
      "suspected_issue": "Pool size may be too large for MVR, but acceptable",
      "mvr_critical": true,
      "enabled_by_default": true
    },
    {
      "id": "http-connection-pool",
      "file": "llm_provider.py",
      "line": 43,
      "construct": "HTTPAdapter pool",
      "type": "connection_pool",
      "purpose": "HTTP connection pooling for Ollama",
      "owner": "get_ollama_client()",
      "count": "22 connections (global_limit + 2)",
      "shared_resources": ["Ollama HTTP endpoint"],
      "synchronization": "requests library internal",
      "suspected_issue": "Pool size (22) may exceed actual concurrency (16-20), wastes memory",
      "mvr_critical": false,
      "enabled_by_default": true
    }
  ],
  "summary": {
    "total_constructs": 17,
    "thread_pools": 5,
    "threads": 4,
    "queues": 2,
    "semaphores": 5,
    "locks": 3,
    "events": 1,
    "connection_pools": 2,
    "async_functions": 15,
    "mvr_critical": 8,
    "enabled_by_default": 12,
    "disabled_by_default": 5
  },
  "suspected_issues": [
    "Nested ThreadPoolExecutors create 64+ threads (4 workers Ã— 16 chunks)",
    "Ollama concurrency limit (16) may be too high for local instance",
    "HTTP pool size (22) exceeds semaphore limit (20)",
    "Async endpoints call sync code, no true async benefit",
    "Adaptive worker scaling adds complexity, may oversubscribe",
    "Multiple ThreadPoolExecutors could be unified"
  ]
}

